# Projet-ENI

## Arborescence du projet

```bash
Projet-ENI/
‚îú‚îÄ‚îÄ backend/                          # Backend Node.js (API REST)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                   # Image Docker du backend
‚îÇ   ‚îú‚îÄ‚îÄ src/                         # Code source de l'API
‚îú‚îÄ‚îÄ frontend/                        # Frontend Angular
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                   # Image Docker du frontend
‚îÇ   ‚îî‚îÄ‚îÄ src/                         # Code source Angular
‚îÇ       ‚îú‚îÄ‚îÄ app/
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yaml              # Orchestration locale des services
‚îÇ
‚îú‚îÄ‚îÄ iac/                             # Infrastructure as Code (Terraform)
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ providers.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îî‚îÄ‚îÄ modules/                     # Modules Terraform r√©utilisables
‚îÇ       ‚îú‚îÄ‚îÄ aks/
‚îÇ       ‚îú‚îÄ‚îÄ keyvault/
‚îÇ       ‚îú‚îÄ‚îÄ mysql/
‚îÇ       ‚îî‚îÄ‚îÄ network/
‚îÇ
‚îú‚îÄ‚îÄ k8s/                             # Manifests Kubernetes
‚îÇ   ‚îú‚îÄ‚îÄ akv-secret.yaml              # Secret via Azure Key Vault
‚îÇ   ‚îú‚îÄ‚îÄ backend-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ frontend-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml                 # Ingress g√©n√©ral
‚îÇ   ‚îî‚îÄ‚îÄ grafana-ingress.yaml
‚îÇ
‚îú‚îÄ‚îÄ monitoring/                      # Observabilit√© (Grafana, Prometheus)
‚îÇ   ‚îî‚îÄ‚îÄ values.yaml                  # Configuration Helm
‚îÇ
‚îî‚îÄ‚îÄ README.md                        # Documentation g√©n√©rale du projet
```

## Fonctionnement de l‚Äôapplication

L‚Äôapplication suit une architecture classique **3-tiers** avec un frontal web, une API backend et une base de donn√©es.

```mermaid
flowchart TD
    User[Utilisateur Web] --> Ingress[Ingress Controller]
    Ingress --> Frontend[Frontend Angular - Docker]
    Ingress --> Backend[Backend Node.js/Express - Docker]
    Backend --> DB[(Azure Database for MySQL flexible server)]
```
Utilisateur Web : acc√®de √† l‚Äôapplication via un navigateur, en HTTPS, via l‚ÄôIngress Controller de Kubernetes.

Frontend Angular : fournit l‚Äôinterface utilisateur et envoie des requ√™tes HTTP/REST vers le backend.

Backend Node.js/Express : expose une API REST qui g√®re la logique m√©tier (ex. gestion de t√¢ches). Il utilise Sequelize pour communiquer avec la base.

Base MySQL : stocke de mani√®re persistante les donn√©es applicatives. En local, elle est lanc√©e en conteneur Docker ; en production, elle repose sur Azure Database for MySQL flexible server
avec un endpoint priv√©.

Cette s√©paration permet :

Une scalabilit√© ind√©pendante du front et du back (chaque composant peut √™tre r√©pliqu√© s√©par√©ment dans AKS).

Une s√©curit√© renforc√©e (la base de donn√©es n‚Äôest jamais expos√©e publiquement).

Une portabilit√© gr√¢ce aux conteneurs Docker, de l‚Äôenvironnement local jusqu‚Äôau cloud.

# D√©marrage de l'application en local avec Docker

Pour faciliter le d√©veloppement et les tests, l‚Äôapplication peut √™tre ex√©cut√©e int√©gralement en local gr√¢ce √† **Docker** et **docker-compose**.  
Cela permet de reproduire un environnement proche de la production, avec les trois briques principales : **frontend**, **backend** et **base de donn√©es**.
---
### Frontend

Le frontend est une application **Angular** conteneuris√©e. Le `Dockerfile` installe Angular CLI et lance le serveur de d√©veloppement sur `0.0.0.0` pour √™tre accessible depuis l‚Äôext√©rieur du conteneur :

```Dockerfile
FROM node:24-alpine

WORKDIR /usr/src/app

COPY . /usr/src/app

RUN npm install -g @angular/cli
RUN npm install

CMD ["ng", "serve", "--host", "0.0.0.0"]
````

Construction de l‚Äôimage :

```bash
docker build -t projet-eni-frontend .
```
---
### Backend

Le backend est une API **Node.js / Express** utilisant **Sequelize** pour communiquer avec la base de donn√©es.
Le `Dockerfile` expose le port `3000` et d√©marre le serveur via npm :

```Dockerfile
FROM node:24-alpine

WORKDIR /usr/src/app

COPY . .

RUN npm install

EXPOSE 3000

ENTRYPOINT ["npm", "run", "start"]
```

Construction de l‚Äôimage :

```bash
docker build -t projet-eni-backend .
```

---

### Base de donn√©es & orchestration docker-compose

L‚Äôensemble est orchestr√© avec **docker-compose**, qui d√©ploie √©galement la base **MariaDB**.
La base est initialis√©e automatiquement au premier lancement gr√¢ce au script SQL `scriptSQL.sql` (pr√©sent dans le dossier `backend`).

```yaml
version: '3'
services:
  database:
    image: mariadb
    environment:
      MYSQL_DATABASE: todolist_db
      MARIADB_ALLOW_EMPTY_ROOT_PASSWORD: "yes"
    volumes:
      - db_data:/var/lib/mysql
      - ./backend/scriptSQL.sql:/docker-entrypoint-initdb.d/scriptSQL.sql
    ports:
      - "3306:3306"

  frontend:
    image: frontend-projet-eni
    ports:
      - "4200:4200"

  backend:
    image: backend-projet-eni
    env_file: .env
    ports:
      - "3000:3000"

volumes:
  db_data:
```

Fichier `.env` utilis√© par le backend :

```
DB_HOST=database
DB_USER=root
DB_PASSWORD=
DB_NAME=todolist_db
DB_DIALECT=mysql
PORT=3000
```
---
### Lancement de l‚Äôapplication

Pour lancer toute la stack (frontend + backend + base de donn√©es) :

```bash
docker-compose up -d
```

* Frontend accessible sur : üëâ [http://localhost:4200](http://localhost:4200)
* Backend API sur : üëâ [http://localhost:3000/api](http://localhost:3000/api)
* Base de donn√©es MariaDB expos√©e sur : `localhost:3306`

---

### R√©sum√© du fonctionnement en local

1. **docker-compose** d√©marre les 3 services (`frontend`, `backend`, `database`).
2. Au d√©marrage, **MariaDB** ex√©cute automatiquement le script `scriptSQL.sql` pour initialiser la base.
3. Le **backend** se connecte √† la base via les variables du fichier `.env`.
4. Le **frontend Angular** appelle les endpoints REST du backend sur `http://localhost:3000/api`.
5. L‚Äôutilisateur acc√®de √† l‚Äôinterface Angular via son navigateur (`localhost:4200`).

Ce workflow permet de d√©velopper, tester et d√©boguer localement avant de d√©ployer sur AKS.

# Provisionnement sur Azure

## 1. Infrastructure Azure

L‚Äôinfrastructure repose sur plusieurs briques Azure provisionn√©es par **Terraform** :

### Ressources principales

- **Resource Group** d√©di√© au projet (`rg-projet-eni`)
- **Virtual Network (VNet)** avec un sous-r√©seau isol√© pour AKS
- **Azure Kubernetes Service (AKS)** en mode **System Assigned Identity**
- **Azure Database for MariaDB** (service manag√©, non expos√© publiquement)
- **Azure Key Vault** pour le stockage s√©curis√© des secrets
- **Azure Private DNS Zone** pour la r√©solution interne des services PaaS

### Topologie r√©seau

- Le cluster AKS est d√©ploy√© dans un **VNet d√©di√©**.  
- La base MariaDB est **priv√©e** et reli√©e au VNet via une **Private Endpoint**.  
- Une **zone DNS priv√©e** (`mysql.database.azure.com`) est associ√©e pour permettre la r√©solution transparente dans AKS.  
- Les pods du cluster utilisent **Azure CNI** pour b√©n√©ficier d‚Äôadresses IP r√©seau r√©elles, ce qui facilite la communication priv√©e avec MariaDB.  

---

## 2. Provisionnement Terraform

L‚Äôinfrastructure est d√©crite en **Terraform** dans le dossier `iac/`.

### a. Cluster AKS

```terraform
resource "azurerm_kubernetes_cluster" "k8s" {
  name                = var.cluster_name
  location            = var.location
  resource_group_name = var.resource_group_name
  dns_prefix          = var.dns_prefix

  identity {
    type = "SystemAssigned"
  }

  default_node_pool {
    name           = "default"
    node_count     = 2
    vm_size        = "Standard_B2s"
    vnet_subnet_id = var.subnet_id
  }

  network_profile {
    network_plugin = "azure"
    network_policy = "calico"
    service_cidr   = "10.0.0.0/16"
    dns_service_ip = "10.0.0.10"
  }

  role_based_access_control_enabled = true
}
````

Caract√©ristiques :

* **VM Standard\_B2s** pour le node pool par d√©faut (2 n≈ìuds).
* **Azure CNI + Calico** pour combiner scalabilit√© et Network Policies.
* **RBAC** activ√© (int√©gration Azure AD possible pour la gestion des acc√®s).

### b. R√©seau et DNS

```terraform
resource "azurerm_virtual_network" "vnet" {
  name                = "aks-vnet"
  resource_group_name = var.resource_group_name
  location            = var.location
  address_space       = ["10.1.0.0/16"]
}

resource "azurerm_subnet" "aks" {
  name                 = "aks-subnet"
  resource_group_name  = var.resource_group_name
  virtual_network_name = azurerm_virtual_network.vnet.name
  address_prefixes     = ["10.1.1.0/24"]
}
```

* Subnet d√©di√© pour isoler les workloads.
* Association avec la **zone DNS priv√©e** pour MariaDB.

### c. Gestion des secrets avec Azure Key Vault

```terraform
resource "azurerm_key_vault" "kv" {
  name                = "kv-projet-eni"
  location            = var.location
  resource_group_name = var.resource_group_name
  tenant_id           = var.tenant_id
  sku_name            = "standard"
}
```

Les secrets (ex. mot de passe DB) sont stock√©s dans le Key Vault et mont√©s dans Kubernetes via le **Secrets Store CSI Driver**.

---

## 3. D√©ploiement Kubernetes

Une fois AKS provisionn√©, les workloads applicatifs sont d√©ploy√©s avec des **manifests Kubernetes** :

* **Deployments** pour `frontend` et `backend`
* **Services ClusterIP** pour l‚Äôinterconnexion interne
* **Ingress Controller (NGINX)** pour exposer les applications en HTTPS
* **ConfigMaps et Secrets** pour injecter la configuration et les credentials depuis Key Vault

Exemple de SecretProviderClass pour MariaDB :

```yaml
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: azure-kv-mariadb
spec:
  provider: azure
  parameters:
    usePodIdentity: "false"
    useVMManagedIdentity: "true"
    userAssignedIdentityID: "<CLIENT_ID_MI>"
    keyvaultName: "<KV_NAME>"
    objects: |
      array:
        - |
          objectName: mysql-password
          objectType: secret
    tenantId: "<TENANT_ID>"
```

---

## 4. CI/CD GitHub Actions

La cha√Æne CI/CD est d√©finie dans `.github/workflows/` :

* **Trigger** : push sur la branche `main`
* **Jobs** :

    * Build de l‚Äôimage Docker (`frontend` et `backend`)
    * Tests unitaires
    * Push sur Docker Hub
* **Secrets CI/CD** : `DOCKER_USERNAME`, `DOCKER_PASSWORD` stock√©s dans GitHub

Cela garantit que chaque modification du code produit imm√©diatement une image Docker pr√™te pour le d√©ploiement sur AKS.

---

## 5. Monitoring et observabilit√©

Le cluster est √©quip√© d‚Äôune **stack de supervision** :

* **Prometheus** : collecte des m√©triques Kubernetes et applicatives
* **Grafana** : dashboards et alertes personnalis√©es
* **Ingress s√©curis√© en HTTPS** avec Cert-Manager + Let‚Äôs Encrypt

Exemple d‚Äôextrait Helm values pour Grafana :

```yaml
grafana:
  adminUser: "admin"
  adminPasswordExistingSecret: "grafana-admin"
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - grafana.mydomain.com
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.mydomain.com
```
